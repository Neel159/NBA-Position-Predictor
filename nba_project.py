# -*- coding: utf-8 -*-
"""NBA_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KwmqFP0zeY7X0mg6WGFY-0C5QCKzgTiu

# Data Retrieval
"""

import pandas as pd
import numpy as np

"""There are 4 different files that I'm going to load in from the dataset from Kaggle. https://www.kaggle.com/datasets/nicklauskim/nba-per-game-stats-201920. One is for per 36 min stats, one is for shooting stats, one is for advanced stats, and the last is for per possession stats."""

p36_df = pd.read_csv("https://raw.githubusercontent.com/jgalbers12/nba2017-18/main/nba_2020_per_minute.csv") # per 36 minute data
shooting_df = pd.read_csv("https://raw.githubusercontent.com/jgalbers12/nba2017-18/main/nba_2020_shooting.csv") # shooting stats data
adv_df = pd.read_csv("https://raw.githubusercontent.com/jgalbers12/nba2017-18/main/nba_2020_advanced.csv") # advanced stats data
poss_df = pd.read_csv("https://raw.githubusercontent.com/jgalbers12/nba2017-18/main/nba_2020_per_poss.csv") # per possession data

poss_df.describe()

"""# Merging Data

I'm deciding to use the per possession stats, the advanced stats, and the shooting stats in one dataframe. I'm choosing the per possession stats because these account for differences in pace of play (number of possessions per unit of time). Whereas per 36 stats scale based on the amount of time each player is on the court. These are both better than per game stats since some players play a lot more time and possessions than others.
"""

#testing merge
a = p36_df.loc[0:10,["Player"]]
b = shooting_df.loc[0:10,["Player"]]
c = adv_df.loc[0:10,["Player","Pos"]]
a.merge(b.merge(c))

all_df = poss_df.merge(shooting_df.merge(adv_df)) # merge the three dataframes
all_df.head()

all_df.columns

"""# Dropping features/Data Exploration

There are a few players with hybrid positions in the dataset. There are so few that I'm going to drop them since they will only cause problems with the models. I'm also going to drop players who didn't play at least 500 minutes. This is so that we get a sample with players who have played enough for their stats to be representative of their average play.
"""

all_df = all_df.drop(all_df[all_df.Pos.isin(["SF-PF","SF-SG","C-PF","PF-C"])].index)

all_df = all_df.loc[all_df['MP']>500]

"""Let's look at the mean stats by position to get a feel for what columns we can drop."""

all_df.iloc[:, :23].groupby("Pos").mean()

all_df.iloc[:, 24:43].groupby(all_df.Pos).mean()

all_df.iloc[:, 43:63].groupby(all_df.Pos).mean()

all_df.iloc[:, 64:].groupby(all_df.Pos).mean()

players = all_df.Player # grab the player column for later

"""I'm going to drop features that I think are not going to be useful. Alot of the features are obviously not useful (name of player, team name). Most of the others are duplicate of other columns or are covered by other columns. For example 'FT' is free throws made but this is covered by 'FTA' (free throw attempts) and FT% (percentage of free throws made)."""

#make list of features to drop
drop_features = ['Player','WS','Att. Heaves','# Heaves','2P%','3P%','TRB','2P','3P','FG','GS','Tm','Dist.','Age', 'G', 'GS', 'Tm', 'FG', 'FGA', 'FG%', 'FT']

drop_features = [i.strip() for i in drop_features]

len(drop_features)

all_df = all_df.drop(drop_features, axis=1)

all_df.describe()

for col in all_df:
  print(col, all_df[col].isna().sum())

"""I'm going to fill null elements with zero. Null elements are all shooting percentage and proportion stats. If they are null it means that the player did not attempt a shot of the relevant category. I think zero is the most obvious choice to fill these since if these players are not attempting any shots of a certain type, we can assume they are very poor at that shot."""

all_df = all_df.fillna(0)

"""Scale the data. I started by just doing standard scaling (std dev = 1, mean = 0). A lot of the data is skewed right or has a large dist at the minimum value. """

std_df = all_df.copy()
  
# apply normalization techniques
for column in std_df.columns:
  if std_df[column].dtypes != object:
    std_df[column] = (std_df[column] - std_df[column].mean()) / std_df[column].std()    
  
# view normalized data   
display(std_df)

std_df.describe()

"""Lets plot the data in a bunch of histograms to see how it is distributed."""

std_df.plot.hist(subplots=True, layout = (8,7), figsize = (40,35), bins=50)

"""The data is mostly distributed normally. Some of the features are pretty skewed but I don't think this will be a big deal since we will be using models that don't assume this."""

std_df['Pos'].value_counts()

"""# KNN

Let's try KNN first. I will use test_size of 0.25 since this leaves about 100 players in the testing split. I could see an argument for a larger portion of players in the testing split. I will change it if the results are not consistent.
"""

X = std_df.drop(columns=["Pos"])
y = std_df.Pos

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)

from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=10)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

pd.crosstab(y_test, y_pred, margins=True)

"""We can see that the model tends to over predict SG's and probably PF's. The model is very conservative when it comes to predicting SF. This probably means that SF's are very spread out in the feature space."""

from sklearn import metrics

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

print("Balanced Accuracy:", metrics.balanced_accuracy_score(y_test, y_pred)) # try balanced accuracy

"""The dataset isn't so imbalanced that using balanced accuracy seems necessary.

Find best value for k.
"""

import matplotlib.pyplot as plt

X = std_df.drop(columns=["Pos"])
y = std_df.Pos

K = range(2,25) # values for k to test

avg_scores = np.zeros(23) # average scores for each k


for j in range(10):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=j) # test random states 0-9
  scores = [] # keep track of scores for each iteration
  best = 0 # best k
  max = 0 # max accuracy score
  for x in K:
    knn = KNeighborsClassifier(n_neighbors=x)
    knn.fit(X_train,y_train)  
    y_pred = knn.predict(X_test)
    ac = metrics.accuracy_score(y_test, y_pred)
    print(f'k:{x} , acc:{ac}') # print accuracy for each k
    scores.append(ac)
    if (ac>max): # find best k
      best = x
      max = ac
  avg_scores = avg_scores + scores

  print(f'Best K: {best}')
  plt.plot(K, scores, 'bx-') # plot scores for each k
  plt.xlabel('Values of K') 
  plt.ylabel('Accuracy Scores') 
  plt.title(f'Random State {j}')
  plt.show()
print(f'average scores: {[i for i in enumerate(avg_scores/10, 2)]}') # print average accuracy for each k

"""The best value for k looks to be 11. Becuase the dataset is smaller, we can see that we don't get a consistent best k between splits. It looks like we definitly want k>3 but there isn't an obvious best k here.

# SVM

Now I want to try support vector machine. I'm not sure if this will be a good model since there doesn't appear to be obvious groups in the data. This should outperform KNN though since it has the ability to find more complex groups by proojecting to higher dimensions.

I found that the best kernal to use is 'radial basis function', so I will use that here.
"""

from sklearn import svm

avg_accuracy = 0

avg_pre = np.zeros(5)
avg_rec = np.zeros(5)

for i in range(10):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i)
  clf = svm.SVC()
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  acc = metrics.accuracy_score(y_test, y_pred)
  pre, rec, fs, sup = metrics.precision_recall_fscore_support(y_test, y_pred)
  print(f"r_state {i}: accuracy={acc}, precision={pre}, recall={rec}")
  avg_accuracy += acc
  avg_pre += pre
  avg_rec += rec

  

print(f'average_accuracy={avg_accuracy/10} average_precision={avg_pre/10} average_recall={avg_rec/10}')

"""Average accuracy for SVM is about 62%. Average Precision: C: 79%, PF: 47%, PG: 78%, SF: 50%, SG: 56%. Average Recall: C: 80%, PF: 55%, PG: 79%, SF: 26%, SG: 71%. This model is much better at predicting correctly when the player is a center or point guard. The model over predicts shooting guards and power forwards and under predicts small forwards. Small forwards have a very low recall and moderate precision meaning the model is consistently not correctly predicting actual small forwards as being small forwards. We can see this in the crosstab for the last round of the model. The model only predicted 5 small forwards but there were 23 in the test split."""

pd.crosstab(y_test, y_pred, margins=True)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score # quicker than running a bunch of random splits
dtc = DecisionTreeClassifier(criterion='gini', min_samples_split=8, random_state=1)
scores = cross_val_score(dtc, X, y, cv=5)
print(scores)
print(f'average: {scores.mean()}')

from sklearn.ensemble import RandomForestClassifier
for i in range(2,20,2):
  rfc = RandomForestClassifier(random_state=5, min_samples_split=i)
  scores = cross_val_score(rfc, X, y, cv=5)
  print(f"min_samples{i}:")
  print(scores, scores.sum()/5)

"""This is a method for determining the most important features in the dataset. We use the ExtraTreesClassifier and the feature_importances_ method to determine how much the classifier is using each feature to build the model. We can see that assists and rebounding stats are very important whereas the 'all in one' metrics like VORP and BPM are not very important. I wanted to use this to find features to drop, but I'm not seeing any that really stick out. Technique found at: https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e"""

data = std_df
X = data.drop(columns=['Pos']) #independent columns
y = data.Pos   #target column
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(X,y)
print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers
#plot graph
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nsmallest(60).plot(kind='barh', figsize=(10,10))
plt.show()

"""# PCA

I wan't to decrease the dimensionality of the data using PCA to see if this helps with accuracy. If we have overfitting, this should help.
"""

from sklearn.decomposition import PCA
X = std_df.drop(columns=["Pos"])
y = std_df.Pos
pca = PCA() # principal component class
pca.fit(X) # fit to data
expl_var = pca.explained_variance_ratio_ # get the explained variance list
# get array of sum of explained variance at n components
expl_var_n_comp = np.array([sum(expl_var[0:i+1]) for i,n in enumerate(expl_var)])

plt.figure(figsize=(8,6)) #create cumulative proportion of variance plot
plt.plot(range(1,len(expl_var_n_comp)+1), expl_var_n_comp*100,'o-')
plt.axis([0, len(expl_var_n_comp)+1, 0, 100])
plt.xlabel('Number of PCA Components Included')
plt.ylabel('Percentage of variance explained (%)')

"""It looks like we'll need about 15 principal components to get 90% of the variance. If we use the PCA data with on SVM, we can see that the accuracy peaks at 14 principal components which is close to 90% variance explained."""

for n in range(5,21):
  X = std_df.drop(columns=["Pos"])
  y = std_df.Pos
  pca = PCA(n_components=n)
  pca.fit(X) # fit model
  principalComponents = pca.fit_transform(X) # fit transform to get pca data
  pca_df = pd.DataFrame(data = principalComponents # make pca dataframe
              , columns = ['principal component' + str(x) for x in range(1,n+1)], index=X.index)
  pca_df = pd.concat([pca_df, std_df.Pos], axis=1) # add positions to pca_df
  pca_df = pca_df.rename(columns={0:'Pos'})

  X = pca_df.drop(columns=["Pos"])
  y = pca_df.Pos

  scores=[]

  for i in range(10):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=i)
    clf = svm.SVC()
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    scores.append(metrics.accuracy_score(y_pred, y_test))

  scores = np.array(scores)
  print(f"Number of principal compenents: {n}")
  print(f"Accuracy Scores:")
  print(scores)
  print(f'avg score: {scores.mean()}')

"""# KNN (PCA)

Now let's see if the PCA data performs better on the KNN model.
"""

from sklearn.neighbors import KNeighborsClassifier

features = std_df.drop(columns=["Pos"])
target = std_df.Pos

K = range(2,22)
avg_scores = np.zeros(20)

for i in range(8,18):
  pca = PCA(n_components=i)
  pca.fit(features)
  principalComponents = pca.fit_transform(features)
  pca_df = pd.DataFrame(data = principalComponents
              , columns = ['principal component' + str(x) for x in range(1,i+1)], index=X.index)
  pca_df = pd.concat([pca_df, std_df.Pos], axis=1)
  pca_df = pca_df.rename(columns={0:'Pos'})

  X = pca_df.drop(columns=["Pos"])
  y = pca_df.Pos

  avg_scores = np.zeros(20)

  for j in range(10):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=j)
    scores = []
    for x in K:
      knn = KNeighborsClassifier(n_neighbors=x)
      knn.fit(X_train,y_train)  
      y_pred = knn.predict(X_test)
      ac = metrics.accuracy_score(y_test, y_pred)
      scores.append(ac)
    avg_scores = avg_scores + scores

  print(f'average scores (PCs = {i}): {[i for i in enumerate(avg_scores/10, 2)]}')

"""Accuracy is best around 12 principal components for KNN. The model performs similarly for k > 3. It looks like the best we could consistly do with KNN is around 55% accuracy. This isn't the best method for analyzing the data, but I just wanted to see if KNN has the potential to perform better on the PCA data.

# PCA Feature Exploration
"""

pca = PCA(n_components=14)
pca.fit(features)
principalComponents = pca.fit_transform(features)
pca_df = pd.DataFrame(data = principalComponents
            , columns = ['principal component' + str(x) for x in range(1,15)], index=features.index)
pca_df = pd.concat([pca_df, std_df.Pos], axis=1)
pca_df = pca_df.rename(columns={0:'Pos'})

X = pca_df.drop(columns=["Pos"])
y = pca_df.Pos

"""Let's plot the principal components against each other to see how well the positions are spread out. I'm going to only do the first 5 since the principal components become less important the higher the number."""

features = X.to_numpy()
target = y.to_numpy()
target_names = ["PG", "SG", "SF", "PF", "C"]
def pos_to_int(x): # function to return int from position name, need this for colors
  if x == "PG":
    return 0
  elif x == "SG":
    return 1
  elif x == "SF":
    return 2
  elif x == "PF":
    return 3
  else:
    return 4
vfunc = np.vectorize(pos_to_int) # vectorize function
target = vfunc(target) # apply to target
colors = ['red', 'green', 'blue', 'purple', 'orange']

fig = plt.figure(figsize=(30,30)) # plot first 5 PC's against each other
for i in range(5):
  for j in range(i+1,5): # only want unique combinations of the PC's
    for t, c in zip(target_names,colors):
      indicesToKeep = pca_df['Pos'] == t # get array of indices where position is t
      ax = fig.add_subplot(4,4,(j-1)*4+i+1)
      ax.scatter(pca_df.loc[indicesToKeep, pca_df.columns[i]]
               , pca_df.loc[indicesToKeep, pca_df.columns[j]]
               , c = c)
      ax.set_title(f"Prin Comp {j+1} vs Prin Comp {i+1}")
      
fig.legend(target_names, loc='upper left', fontsize='large')

"""Centers and point guards are usually towards the outside of the main grouping of point. Shooting guards, small forwards and power forwards are all pretty intermixed towards the center of the grouping. Small forwards in particular appear to be spread out relatively evenly from the middle of the plot.

Now we want to see how each feature is contributing to each principal component. We can use the components_ attribute of the PCA class to see this. components_ gives the directions of each component of pca in the original feature space. We get a number for each feature for each component. This number tells us how much that feature contributes to the value for that component. We want to find the features that contribute most for each component. Then we can use some of our intuition about basketball to define each principal component as being indicative of some set of skills.
"""

[i for i in zip(std_df.columns[1:], pca.components_.T)] # get direction for each component

abs_pca_comp = np.abs(pca.components_) # find abs of components
sorted_components_i = np.argsort(abs_pca_comp) # sort components
i_to_col = dict(enumerate(std_df.columns[1:])) # get the feature names
sorted_components = np.vectorize(i_to_col.get)(sorted_components_i) # sorted feature names
for i in range(14): # print most important features for each component
  print(f"Top 10 contributors to principal component {i+1}")
  print(np.flip(sorted_components[i][-10:]), "\n") # need to flip since sorted min to max

"""My interpretation of what each principal component is rating.

1. Paint play (rebounds, dunks, layups)
2. Creation (assists, points, high usage)
3. Inverse offensive efficiency (low TS, low ORtg, high turnovers)
4. Defensive efficiency (high defensive stats)
5. Offensive tendency (high ORtg, Low DRtg, Low DRB, high TS%)
6. Inverse mid-range tendency (low mid-range, high 3P, high inside shots)
7. Passing/ballhandling (high turnovers and assists)
8. Inverse 3Pt efficiency (low 3pt%)
9. Bench tendency (low minutes)
10. Floater efficiency/inverse low 2P efficiency
11. Mid range efficiency/inverse close and long midrange efficiency
12. "3 and D"
13. Foul drawing
14. Long midrange tendency/steals

These are not perfect categories as they don't encapsulate everything that each PC is based on, just the most important features and their direction. They should help us make some generalizations about the groups of players though.
"""

pca_df.groupby("Pos").mean()

"""We can see that centers are high in the 'Post Play' component and point guards have a high 'Creation' component which makes sense.

# Decision Tree and Random Forest

I want to try two classification models, decision tree and random forest on the pca data. I'm expecting these to give similar results to SVM with random forest outperforming decision tree.
"""

X = std_df.drop(columns=["Pos"]) # redo pca with 14 components
y = std_df.Pos
pca = PCA(n_components=14)
pca.fit(X)
principalComponents = pca.fit_transform(X)
pca_df = pd.DataFrame(data = principalComponents
            , columns = ['principal component' + str(x) for x in range(1,15)], index=X.index)
pca_df = pd.concat([pca_df, std_df.Pos], axis=1)
pca_df = pca_df.rename(columns={0:'Pos'})

X = pca_df.drop(columns=["Pos"])
y = pca_df.Pos

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score # quicker than running a bunch of random splits
dtc = DecisionTreeClassifier(criterion='gini', min_samples_split=8, random_state=1)
scores = cross_val_score(dtc, X, y, cv=5)
print(scores)
print(f'average: {scores.mean()}')

"""Decision tree looks like it will perform pretty poorly. Let's tree random forest."""

from sklearn.ensemble import RandomForestClassifier
for i in range(2,20,2):
  rfc = RandomForestClassifier(random_state=5, min_samples_split=i)
  scores = cross_val_score(rfc, X, y, cv=5)
  print(f"min_samples{i}:")
  print(scores, scores.sum()/5)

"""This performs much better than decision tree but not as well as SVM, so I'm not going to dig deep into the results for now.

# Unsupervised Clustering

Classification of players went alright. I think we learned that apart from point guards and centers, it is very difficult to classify players accurately using their stats. Now I want to see how an upsupervised model would group the players.
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

X = pca_df.drop(columns=["Pos"])

for i in range(2, 20):
  km = KMeans(n_clusters=i, max_iter=1000, tol=0.01) # KMeans for 2-19 clusters
  km.fit(X)
  ss = silhouette_score(X, km.labels_)
  print(f'cluster: {i}, silhouette: {ss}')

"""KMeans looks like its not going to give great results. I'm going to try another clustering method called agglomerative heirarchical clustering that I found online. This method uses some 'distance' function and finds clusters by finding the two clusters with the minimum 'distance' between them, making them a single cluster, and repeating until there is one cluster. We can make a graph showing how the clusters were formed called a dendrogram. Later, we can set a distance limit and once we reach that distance in the algorithm, we will have our clusters."""

from sklearn.cluster import AgglomerativeClustering

X = std_df.drop(columns=["Pos"])

for i in range(1, 20):
  ac = AgglomerativeClustering(None, distance_threshold=i) # get a clustering for distances 1-20
  ac.fit(X)
  ss = silhouette_score(X, ac.labels_)
  print(f'clusters: {ac.n_clusters_}, silhouette: {ss}')

"""We can see that the algorithm starts with a cluster for each player. As we increase the distance threshold, the number of clusters decreases."""

from scipy.cluster.hierarchy import dendrogram, linkage

X = std_df.drop(columns=["Pos"])
Z = linkage(X, 'ward') # get the clustering of X using Ward's min variance as distance

"""Let's use std_df for now. linkage from scipy.cluster.hierarchy performs the clustering algorithm and gives a matrix that details which clusters were combined, the 'distance' between them, and size of the resulting cluster. I tried every distance formula and found 'ward' to be the best. It uses the minimum increase in within cluster variance to decide which clusters it will combine in each step."""

Z

"""We can see that the algorithm stats by grouping single players into clusters of 2. At the last step, we have a single cluster of all the players."""

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist

c, coph_dists = cophenet(Z, pdist(X))
c

"""The cophenetic correlation tells us how well our dendrogram preserves the original distances between the datapoints. A value closer to one is better."""

def index_to_player(i): # function to return players using index
  return players.iloc[i]

plt.figure(figsize=(50, 30))
plt.title('NBA Clustering Dendrogram')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(
    Z,
    leaf_rotation=90.,  # rotates the x axis labels
    leaf_font_size=7.5,  # font size for the x axis labels
    color_threshold=20, # distance at which we cutoff to make groups
    leaf_label_func=index_to_player  # function that gives us player names at leaves
)
plt.show()

"""From my experience watching basketball, this is not a bad grouping. It is especially good at grouping centers and 'star' players (players that have the ball a lot). Additionally, I think you could make a case that there is a decent label for each group."""

X = std_df.drop(columns=["Pos"])

pca = PCA(n_components=14)
pca.fit(X)
principalComponents = pca.fit_transform(X)
pca_df = pd.DataFrame(data = principalComponents
            , columns = ['principal component' + str(x) for x in range(1,15)], index=X.index)
pca_df = pd.concat([pca_df, std_df.Pos], axis=1)
pca_df = pca_df.rename(columns={0:'Pos'})

X = pca_df.drop(columns=["Pos"])
Z = linkage(X, 'ward')

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist

c, coph_dists = cophenet(Z, pdist(X))
c

plt.figure(figsize=(50, 30))
plt.title('NBA Clustering Dendrogram (PCA)')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(
    Z,
    leaf_rotation=90.,  # rotates the x axis labels
    leaf_font_size=8., # font size for the x axis labels
    color_threshold=21,
    leaf_label_func=index_to_player
)
plt.show()

"""PCA and standardized data have very few differences if we use the same parameters for the dendogram. I like this dendrogram a bit more though. The groups are a bit more even and I think it is a bit easier to define what type of player is in each group.

Using the above dendrogram, we can see that the players were split into 11 groups when we cut off the grouping at a distance of 21.
"""

plt.figure(figsize=(20, 15))
plt.title('Simplified NBA Clustering Dendrogram')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(
    Z,
    p=25,
    truncate_mode='lastp',
    leaf_rotation=90.,
    leaf_font_size=8.,  # font size for the x axis labels
    show_contracted=True,
    color_threshold=20
)
plt.show()

"""Now we can use fcluster to get an array that gives the group for each player. We will use the same distance (21) as in the dendrogram and we should get 11 groups."""

from scipy.cluster.hierarchy import fcluster
max_d = 21
clusters = fcluster(Z, max_d, criterion='distance')
clusters

pca_ri_df = pca_df.reset_index() # reset index of pca so that we can add clusters

pca_ri_df

pca_w_clst_df = pd.concat((pca_df, pd.Series(clusters, index=pca_df.index, name='cluster'), players), axis=1)
pca_w_clst_df # concatonate our pca_df with the clusters and player names

"""Let's give the first 6 PCA components the attribute names from before so that we can make a good looking heatmap."""

pc_to_att = {'principal component1': 'Paint Play', 'principal component2': 'Creation', 
             'principal component3': 'Inverse Offensive Efficiency', 'principal component4': 'Defensive Efficiency',
             'principal component5': 'Offense/Defense Ratio', 'principal component6': 'Inverse Mid-Range Tendency'}

"""Let's look at the mean component value for each of our clusters."""

grouping_df = pca_w_clst_df.groupby("cluster").mean()
grouping_df = grouping_df.rename(columns=pc_to_att)
grouping_df

"""Here's a heatmap showing clusters and first 6 components."""

import seaborn as sns
sns.heatmap(grouping_df.iloc[:, 0:6])

for i in range(1,12):
  print(f"cluster: {i} {[j for j in pca_w_clst_df['Player'].loc[pca_w_clst_df['cluster']==i]]}")

"""Cluster analysis, our interpretation of groupings:

1. Traditional Centers
2. Smaller, less traditional centers
3. Stretch 4/5s, Offensive bigs
4. Star Players
5. Secondary creators, strong defenders
6. Offensive creators, weak defenders
7. Small Backup Wings
8. Bench Creators/Spark plugs
9. Defensive specialist wings
10. Stretch 4's
11. Knock-down shooters

Now we will plot the first 5 PCA components against each other using a different color for each cluster of players.
"""

X = pca_ri_df.drop(columns=["Pos"])
features = X.to_numpy()
target = clusters
target_names=[i for i in range(1,12)]
colors = ['red', 'green', 'blue', 'purple', 'orange', 'black', 'brown', 'pink', 'lime', 'olive', 'turquoise']

fig = plt.figure(figsize=(30,30))
for i in range(5):
  for j in range(i+1,5):
    for t, c in zip(target_names,colors):
      indicesToKeep = clusters == t
      ax = fig.add_subplot(4,4,(j-1)*4+i+1)
      ax.scatter(pca_df.loc[indicesToKeep, pca_df.columns[i]]
               , pca_df.loc[indicesToKeep, pca_df.columns[j]]
               , c = c)
      ax.set_title(f"Prin Comp {j+1} vs Prin Comp {i+1}")
      
fig.legend(target_names, loc='upper left', fontsize='large')

"""Obviously this plot has much better separation between the clusters compared to the PCA plot from before.

# Add heights

We want to try adding a heights column to the dataset to see if this improves our models. Our expectation is that it will drastically improve the decision tree and random forest since these can better use individual features when classifying. We don't expect KNN or SVM to have significantly better results since a single feature doesn't have a lot of weight in these models.
"""

heights_df = pd.read_csv("https://raw.githubusercontent.com/Neel159/ProgrammingAssignment2/master/C%3A%5CUsers%5CNeel%5CDesktop%5C8.csv")
heights_df.describe(include="all")

all_df = heights_df.merge(shooting_df,on=["Player", 'Pos', 'Age', 'Tm', 'G', 'MP'],how='inner')

all_df = all_df.merge(poss_df, on=["Player", 'Pos', 'Age', 'Tm', 'G', 'MP', 'FG%'], how='inner')

all_df.describe(include='all')

all_df.columns

drop_features_1 = drop_features
drop_features_1.append('position')
drop_features_1.append('season')
drop_features_1.append('salary')
drop_features_1.append('team')

w_ht_df = all_df.loc[all_df['MP']>500]
players = w_ht_df.Player
w_ht_df = w_ht_df.drop(columns=drop_features_1)
w_ht_df = w_ht_df.fillna(0)
std_w_ht_df = w_ht_df.copy()
# apply normalization techniques
for column in std_w_ht_df.columns:
  if std_w_ht_df[column].dtypes != object:
    std_w_ht_df[column] = (std_w_ht_df[column] - std_w_ht_df[column].mean()) / std_w_ht_df[column].std()

std_w_ht_df = std_w_ht_df.drop(std_w_ht_df[std_w_ht_df.Pos.isin(["SF-PF","SF-SG","C-PF","PF-C"])].index)

std_w_ht_df

"""# KNN with heights"""

import matplotlib.pyplot as plt

X = std_w_ht_df.drop(columns=["Pos"])
y = std_w_ht_df.Pos

K = range(2,25)

avg_scores = np.zeros(23)


for j in range(10):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=j)
  scores = []
  best = 0
  max = 0
  for x in K:
    knn = KNeighborsClassifier(n_neighbors=x)
    knn.fit(X_train,y_train)  
    y_pred = knn.predict(X_test)
    ac = metrics.accuracy_score(y_test, y_pred)
    print(f'k:{x} , acc:{ac}')
    scores.append(ac)
    if (ac>max):
      best = x
      max = ac
  avg_scores = avg_scores + scores
  
  print(f'Best K: {best}')
  plt.plot(K, scores, 'bx-') 
  plt.xlabel('Values of K') 
  plt.ylabel('Accuracy Scores') 
  plt.title(f'Random State {j}')
  plt.show() 
print(f'average scores: {[i for i in enumerate(avg_scores/10, 2)]}') # print average accuracy for each k

"""Best K is consistently around 12. The model performs slightly better with the heights data."""

X = std_w_ht_df.drop(columns=["Pos"])
y = std_w_ht_df.Pos
dtc = DecisionTreeClassifier(criterion='gini', min_samples_split=8, random_state=0)
scores = cross_val_score(dtc, X, y, cv=5)
scores

"""Decision tree performs significantly better with the heights data.

Let's get a visual representation of what is going on here.
"""

dtc.fit(X, y)

from sklearn import tree
fig = plt.figure(figsize=(40,40))
_ = tree.plot_tree(dtc, 
                   feature_names=X.columns,  
                   class_names=y.unique(),
                   filled=True,
                   max_depth=3)

"""If zoom in on the figure, we can see that the decision tree is using height to split to data in the first and second layers of nodes. This means that height is one of the most important features in the tree."""

for i in range(2,20,2):
  rfc = RandomForestClassifier(random_state=3, min_samples_split=i)
  scores = cross_val_score(rfc, X, y, cv=5)
  print(f"min_samples{i}:")
  print(scores, scores.sum()/5)

"""Minimum split of about 4 looks like it gives the best results. Let's use this across 10 random states for splits."""

for i in range(1, 11):
  rfc = RandomForestClassifier(random_state=3, min_samples_split=4)
  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=i)
  rfc.fit(X_train,y_train)
  y_pred = rfc.predict(X_test)
  score = metrics.accuracy_score(y_test, y_pred)
  pre, rec, f, s = metrics.precision_recall_fscore_support(y_test, y_pred)
  print(f"split {i}:")
  print(f'accuracy= {score}')
  print(f'precision= {pre}')
  print(f'recall= {rec}')

"""Crosstab for the last split."""

pd.crosstab(y_test, y_pred, margins=True)

"""Random forest also gives much better results when we add in the heights data. """

clf = svm.SVC()
scores = cross_val_score(clf, X, y, cv=5)

print(scores, scores.sum()/5)

"""It looks like SVM gives similar results with the heights data."""